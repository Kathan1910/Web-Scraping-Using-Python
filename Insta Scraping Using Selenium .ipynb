{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb7f18d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'executable_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21664/1567961011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Set the path to your Chrome WebDriver executable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'E:\\Web Scraping\\chromedriver-win64\\chromedriver.exe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Manually input Instagram usernames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'executable_path'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "# Set the path to your Chrome WebDriver executable\n",
    "driver = webdriver.Chrome(executable_path='E:\\Web Scraping\\chromedriver-win64\\chromedriver.exe')\n",
    "\n",
    "# Manually input Instagram usernames\n",
    "manual_usernames = ['patel_dharmik_612', 'yash_parekh0310', 'jerry___7701']\n",
    "\n",
    "# If you want to read usernames from a CSV file, replace the manual_usernames list with CSV reading code:\n",
    "# with open('usernames.csv', 'r') as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     manual_usernames = [row['username'] for row in reader]\n",
    "\n",
    "for username in manual_usernames:\n",
    "    # Open Instagram and navigate to the user's profile\n",
    "    driver.get(f'https://www.instagram.com/{username}/')\n",
    "\n",
    "    # Find the bio element using the provided XPath\n",
    "    bio_element = driver.find_element_by_xpath('//*[@id=\"mount_0_0_or\"]/div/div/div[2]/div/div/div/div[1]/div[1]/div[2]/div[2]/section/main/div/div[1]')\n",
    "\n",
    "    # Extract the bio text\n",
    "    bio = bio_element.text\n",
    "\n",
    "    # Check if the email is present in the bio\n",
    "    if 'your_email@example.com' in bio:\n",
    "        print(f'{username}: Email found in bio')\n",
    "    else:\n",
    "        print(f'{username}: Email not found in bio')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b257d602",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21664/503829997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Create a Chrome WebDriver using the Service object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchrome_driver_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Service' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set the path to your Chrome WebDriver executable\n",
    "chrome_driver_path = 'E:\\Web Scraping\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "# Create a Chrome WebDriver using the Service object\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open Instagram login page\n",
    "driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "\n",
    "# Wait for the login page to load\n",
    "time.sleep(2)\n",
    "\n",
    "# Enter your Instagram username and password\n",
    "your_username = \"kathan_1910\"\n",
    "your_password = \"freekathan\"\n",
    "\n",
    "username_input = driver.find_element_by_name(\"username\")\n",
    "password_input = driver.find_element_by_name(\"password\")\n",
    "\n",
    "username_input.send_keys(your_username)\n",
    "password_input.send_keys(your_password)\n",
    "\n",
    "# Submit the login form\n",
    "password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the login to complete (you can adjust the time as needed)\n",
    "time.sleep(5)\n",
    "\n",
    "# Manually input Instagram usernames or read them from a CSV file\n",
    "manual_usernames = ['patel_dharmik_612', 'yash_parekh0310', 'jerry___7701']\n",
    "\n",
    "# If you want to read usernames from a CSV file, replace the manual_usernames list with CSV reading code:\n",
    "# with open('usernames.csv', 'r') as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     manual_usernames = [row['username'] for row in reader]\n",
    "\n",
    "for username in manual_usernames:\n",
    "    # Open Instagram and navigate to the user's profile\n",
    "    driver.get(f'https://www.instagram.com/{username}/')\n",
    "\n",
    "    # Find the bio element using the provided XPath\n",
    "    bio_element = driver.find_element_by_xpath('//*[@id=\"mount_0_0_or\"]/div/div/div[2]/div/div/div/div[1]/div[1]/div[2]/div[2]/section/main/div/div[1]')\n",
    "\n",
    "    # Extract the bio text\n",
    "    bio = bio_element.text\n",
    "\n",
    "    # Check if the email is present in the bio\n",
    "    if 'your_email@example.com' in bio:\n",
    "        print(f'{username}: Email found in bio')\n",
    "    else:\n",
    "        print(f'{username}: Email not found in bio')\n",
    "\n",
    "# Close the WebDriver when done\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226c2501",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21664/2232605662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Find the bio element using the provided XPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mbio_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//*[@id=\"mount_0_0_or\"]/div/div/div[2]/div/div/div/div[1]/div[1]/div[2]/div[2]/section/main/div/div[1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Extract the bio text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set the path to your Chrome WebDriver executable\n",
    "chrome_driver_path = 'E:\\Web Scraping\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "# Add the directory containing the Chrome WebDriver executable to the PATH environment variable\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.dirname(chrome_driver_path)\n",
    "\n",
    "# Create a Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Manually log in to Instagram using your web browser\n",
    "\n",
    "# After manually logging in, navigate to the Instagram profiles you want to check\n",
    "manual_usernames = ['patel_dharmik_612', 'yash_parekh0310', 'jerry___7701']\n",
    "\n",
    "for username in manual_usernames:\n",
    "    # Open Instagram and navigate to the user's profile\n",
    "    driver.get(f'https://www.instagram.com/{username}/')\n",
    "\n",
    "    # Find the bio element using the provided XPath\n",
    "    bio_element = driver.find_element_by_xpath('//*[@id=\"mount_0_0_or\"]/div/div/div[2]/div/div/div/div[1]/div[1]/div[2]/div[2]/section/main/div/div[1]')\n",
    "\n",
    "    # Extract the bio text\n",
    "    bio = bio_element.text\n",
    "\n",
    "    # Check if the email is present in the bio\n",
    "    if 'your_email@example.com' in bio:\n",
    "        print(f'{username}: Email found in bio')\n",
    "    else:\n",
    "        print(f'{username}: Email not found in bio')\n",
    "\n",
    "# Close the WebDriver when done\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "243bc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bio for jerry___7701: 1,334 Followers, 639 Following, 4 Posts - See Instagram photos and videos from |Wedding By JERRY SONI™️| (@jerry___7701)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace 'username' with the desired Instagram username\n",
    "username = 'jerry___7701'\n",
    "\n",
    "# URL of the Instagram profile\n",
    "url = f'https://www.instagram.com/{username}/'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the bio element\n",
    "bio_element = soup.find('meta', property='og:description')\n",
    "\n",
    "# Extract the content attribute (bio text)\n",
    "if bio_element:\n",
    "    bio_text = bio_element['content']\n",
    "    print(f'Bio for {username}: {bio_text}')\n",
    "else:\n",
    "    print(f'Unable to find bio for {username}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18f59b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bio for jerry___7701:\n",
      "1,334 Followers, 639 Following, 4 Posts - See Instagram photos and videos from |Wedding By JERRY SONI™️| (@jerry___7701)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace 'username' with the desired Instagram username\n",
    "username = 'jerry___7701'\n",
    "\n",
    "# URL of the Instagram profile\n",
    "url = f'https://www.instagram.com/{username}/'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the bio element using the 'og:description' property\n",
    "bio_element = soup.find('meta', property='og:description')\n",
    "\n",
    "# Extract the content attribute (bio text)\n",
    "bio_text = bio_element['content'] if bio_element else ''\n",
    "\n",
    "# Search for specific class names that commonly appear in Instagram profiles\n",
    "common_class_names = ['-vDIg', 'x7a106z']\n",
    "for class_name in common_class_names:\n",
    "    additional_bio_element = soup.find('div', class_=class_name)\n",
    "    if additional_bio_element:\n",
    "        additional_bio_text = additional_bio_element.get_text()\n",
    "        bio_text += '\\n' + additional_bio_text\n",
    "\n",
    "# Print the combined bio text\n",
    "print(f'Bio for {username}:\\n{bio_text.strip()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f9f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bio for patel_dharmik_612:\n",
      "0 Followers, 333 Following, 28 Posts - See Instagram photos and videos from Dharmik Patel (@patel_dharmik_612)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace 'username' with the desired Instagram username\n",
    "username = 'patel_dharmik_612'\n",
    "\n",
    "# URL of the Instagram profile\n",
    "url = f'https://www.instagram.com/{username}/'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the bio element using the 'og:description' property\n",
    "bio_element = soup.find('meta', property='og:description')\n",
    "\n",
    "# Extract the content attribute (bio text)\n",
    "bio_text = bio_element['content'] if bio_element else ''\n",
    "\n",
    "# Search for specific class names that commonly appear in Instagram profiles\n",
    "common_class_names = ['-vDIg', 'x7a106z']\n",
    "for class_name in common_class_names:\n",
    "    additional_bio_element = soup.find('div', class_=class_name)\n",
    "    if additional_bio_element:\n",
    "        additional_bio_text = additional_bio_element.get_text()\n",
    "        bio_text += '\\n' + additional_bio_text\n",
    "\n",
    "# Print the combined bio text\n",
    "print(f'Bio for {username}:\\n{bio_text.strip()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94238bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instagram Bio for jerry___7701:\n",
      "1,334 Followers, 639 Following, 4 Posts - See Instagram photos and videos from |Wedding By JERRY SONI&#x2122;&#xfe0f;| (&#064;jerry___7701)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Replace 'username' with the desired Instagram username\n",
    "username = 'jerry___7701'\n",
    "\n",
    "# URL of the Instagram profile\n",
    "url = f'https://www.instagram.com/{username}/'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the response status code is OK\n",
    "if response.status_code == 200:\n",
    "    # Use regular expressions to extract bio text from the HTML content\n",
    "    bio_pattern = r'<meta content=\"([^\"]+)\" name=\"description\" />'\n",
    "    bio_match = re.search(bio_pattern, response.text)\n",
    "\n",
    "    if bio_match:\n",
    "        bio_text = bio_match.group(1)\n",
    "        print(f'Instagram Bio for {username}:\\n{bio_text}')\n",
    "    else:\n",
    "        print(f'Unable to find bio for {username}')\n",
    "else:\n",
    "    print(f'Failed to fetch Instagram profile for {username}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1794c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instagram Bio for patel_dharmik_612:\n",
      "0 Followers, 333 Following, 28 Posts - See Instagram photos and videos from Dharmik Patel (&#064;patel_dharmik_612)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Replace 'username' with the desired Instagram username\n",
    "username = 'patel_dharmik_612'\n",
    "\n",
    "# URL of the Instagram profile\n",
    "url = f'https://www.instagram.com/{username}/'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the response status code is OK\n",
    "if response.status_code == 200:\n",
    "    # Use regular expressions to extract bio text from the HTML content\n",
    "    bio_pattern = r'<meta content=\"([^\"]+)\" name=\"description\" />'\n",
    "    bio_match = re.search(bio_pattern, response.text)\n",
    "\n",
    "    if bio_match:\n",
    "        bio_text = bio_match.group(1)\n",
    "        print(f'Instagram Bio for {username}:\\n{bio_text}')\n",
    "    else:\n",
    "        print(f'Unable to find bio for {username}')\n",
    "else:\n",
    "    print(f'Failed to fetch Instagram profile for {username}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bda1cfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Web Scraping\\\\Web-Scraping-Using-Python'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8682e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to your Scrapy project directory\n",
    "project_directory = r'E:\\Web Scraping\\Web-Scraping-Using-Python\\instagram_bio_scraping\\instagram_bio_scraping'\n",
    "\n",
    "# Change the current working directory to the project directory\n",
    "os.chdir(project_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81622a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Web Scraping\\\\Web-Scraping-Using-Python\\\\instagram_bio_scraping\\\\instagram_bio_scraping'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c15e9b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'insta_bio_scraper' using template 'basic' \n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider insta_bio_scraper example.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc2944f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class InstagramBioSpider(scrapy.Spider):\n",
    "    name = 'insta_bio_scraper'\n",
    "    allowed_domains = ['instagram.com']\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Check if the 'username' argument is provided when running the spider\n",
    "        username = getattr(self, 'username', None)\n",
    "        if username is None:\n",
    "            self.logger.error(\"You must provide a username argument like: scrapy crawl insta_bio_scraper -a username=your_username\")\n",
    "            return\n",
    "\n",
    "        # Construct the URL for the Instagram profile\n",
    "        url = f'https://www.instagram.com/{username}/'\n",
    "\n",
    "        # Start the scraping process\n",
    "        yield scrapy.Request(url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract the bio text using XPath\n",
    "        bio = response.xpath('//meta[@name=\"description\"]/@content').get()\n",
    "\n",
    "        if bio:\n",
    "            yield {\n",
    "                'username': response.url.split('/')[-2],  # Extract the username from the URL\n",
    "                'bio_text': bio\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7554429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 15:24:04 [scrapy.utils.log] INFO: Scrapy 2.10.0 started (bot: instagram_bio_scraping)\n",
      "2023-09-05 15:24:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1s  1 Nov 2022), cryptography 37.0.4, Platform Windows-10-10.0.19045-SP0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\spiderloader.py\", line 80, in load\n",
      "    return self._spiders[spider_name]\n",
      "KeyError: 'insta_bio_scraper'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\cmdline.py\", line 161, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\cmdline.py\", line 114, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\cmdline.py\", line 169, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 23, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\crawler.py\", line 244, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\crawler.py\", line 280, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\crawler.py\", line 360, in _create_crawler\n",
      "    spidercls = self.spider_loader.load(spidercls)\n",
      "  File \"C:\\Users\\kathan\\anaconda3\\envs\\myenv\\lib\\site-packages\\scrapy\\spiderloader.py\", line 82, in load\n",
      "    raise KeyError(f\"Spider not found: {spider_name}\")\n",
      "KeyError: 'Spider not found: insta_bio_scraper'\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl insta_bio_scraper -a username=jerry___7701 -o bios.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba02fc3",
   "metadata": {},
   "source": [
    "##  Sample Spyder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd08b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'instagram_bio_scraper', using template directory 'C:\\Users\\kathan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    E:\\Web Scraping\\Web-Scraping-Using-Python\\instagram_bio_scraping\\instagram_bio_scraping\\instagram_bio_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd instagram_bio_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject instagram_bio_scraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4376ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Web Scraping\\\\Web-Scraping-Using-Python\\\\instagram_bio_scraping\\\\instagram_bio_scraping'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cda7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!cd instagram_bio_scraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b980b394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Web Scraping\\\\Web-Scraping-Using-Python\\\\instagram_bio_scraping\\\\instagram_bio_scraping'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fe9794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to your desired directory\n",
    "directory_path = r'E:\\Web Scraping\\Web-Scraping-Using-Python\\instagram_bio_scraping\\instagram_bio_scraping\\instagram_bio_scraper\\instagram_bio_scraper'\n",
    "\n",
    "# Change the current working directory to the specified path\n",
    "os.chdir(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afd5801a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Web Scraping\\\\Web-Scraping-Using-Python\\\\instagram_bio_scraping\\\\instagram_bio_scraping\\\\instagram_bio_scraper\\\\instagram_bio_scraper'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afe39ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'my_instagram_bio_scraper' using template 'basic' in module:\n",
      "  instagram_bio_scraper.spiders.my_instagram_bio_scraper\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider my_instagram_bio_scraper instagram.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80c6d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class InstagramBioScraperSpider(scrapy.Spider):\n",
    "    name = 'instagram_bio_scraper'\n",
    "    allowed_domains = ['instagram.com']\n",
    "\n",
    "    def __init__(self, username='', *args, **kwargs):\n",
    "        super(InstagramBioScraperSpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [f'https://www.instagram.com/{username}/']\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, headers={'User-Agent': 'Mozilla/5.0'}, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Check if the page exists\n",
    "        if response.status != 200:\n",
    "            self.logger.error(f'Failed to fetch Instagram page for {response.url}')\n",
    "            return\n",
    "\n",
    "        # Extract the bio text using XPath\n",
    "        bio = response.xpath('//meta[@name=\"description\"]/@content').get()\n",
    "\n",
    "        if bio:\n",
    "            yield {\n",
    "                'username': response.url.split('/')[-2],  # Extract the username from the URL\n",
    "                'bio_text': bio\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fdb1564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 15:30:44 [scrapy.utils.log] INFO: Scrapy 2.10.0 started (bot: instagram_bio_scraper)\n",
      "2023-09-05 15:30:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1s  1 Nov 2022), cryptography 37.0.4, Platform Windows-10-10.0.19045-SP0\n",
      "2023-09-05 15:30:45 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-05 15:30:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'instagram_bio_scraper',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'instagram_bio_scraper.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['instagram_bio_scraper.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-09-05 15:30:45 [asyncio] DEBUG: Using selector: SelectSelector\n",
      "2023-09-05 15:30:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-09-05 15:30:45 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
      "2023-09-05 15:30:45 [scrapy.extensions.telnet] INFO: Telnet Password: c9aedf76700a4388\n",
      "2023-09-05 15:30:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-05 15:30:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-05 15:30:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-05 15:30:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-05 15:30:45 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-05 15:30:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-05 15:30:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-05 15:30:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://instagram.com/robots.txt> (referer: None)\n",
      "2023-09-05 15:30:45 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://instagram.com>\n",
      "2023-09-05 15:30:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-05 15:30:45 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: bios.json\n",
      "2023-09-05 15:30:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 1,\n",
      " 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 1,\n",
      " 'downloader/request_bytes': 228,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 3199,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.318988,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 5, 10, 0, 45, 746699),\n",
      " 'httpcompression/response_bytes': 2190,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'robotstxt/forbidden': 1,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 9, 5, 10, 0, 45, 427711)}\n",
      "2023-09-05 15:30:45 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl my_instagram_bio_scraper -a username=patel_dharmik_612 -o bios.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e3022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "mynev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
